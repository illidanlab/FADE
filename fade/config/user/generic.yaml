# @package _group_
name: generic
loss: xent
optimizer:
  name: sgd
  learning_rate: 0.005  # Local learning rate
  personal_learning_rate: 0.09  # Persionalized learning rate to caculate theta aproximately using K steps
privacy:
  enable: false
#    user_clip_norm: -1  # User-DP clip norm (layer wise)
#    user_dp_sigma: -1  # User-DP noise sigma
batch_size: 20
local_epochs: 20
lamda: 0  # Regularization term
K: 5  # Computation steps on local fine-tune